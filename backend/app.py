from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import os
import subprocess
import threading
import time
import uuid
import json
import requests
from werkzeug.utils import secure_filename
import shutil
import base64
from io import BytesIO
import zipfile

app = Flask(__name__)
CORS(app)

# Configuration
UPLOAD_FOLDER = '/tmp/datasets'
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'webp'}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

# RunPod Configuration - Updated for PyTorch Pod
// config.js
export const RUNPOD_API_KEY = import.meta.env.VITE_RUNPOD_API_KEY;
export const RUNPOD_PYTORCH_POD_URL = import.meta.env.VITE_RUNPOD_PYTORCH_POD_URL;
export const RUNPOD_ENDPOINT_ID = import.meta.env.VITE_RUNPOD_ENDPOINT_ID;
export const RUNPOD_TRAINING_ENDPOINT_ID = import.meta.env.VITE_RUNPOD_TRAINING_ENDPOINT_ID;
export const RUNPOD_BASE_URL = import.meta.env.VITE_RUNPOD_BASE_URL;


# If you want to keep using the serverless SD endpoint:
print("üîß RunPod Configuration:")
print(f"   PyTorch Pod URL: {RUNPOD_PYTORCH_POD_URL}")
print(f"   Serverless SD Endpoint: {RUNPOD_ENDPOINT_ID}")
print(f"   API Key: {RUNPOD_API_KEY[:10]}..." if RUNPOD_API_KEY else "   API Key: Not set")

# Store job status and results
jobs = {}
datasets = {}

class DatasetPipeline:
    """Unified pipeline for LoRA training, image generation, and SAM segmentation"""
    
    def __init__(self, job_id):
        self.job_id = job_id
        self.job = jobs[job_id]
        
    def update_status(self, status, progress, message, **kwargs):
        """Update job status with detailed info"""
        self.job.update({
            'status': status,
            'progress': progress,
            'message': message,
            'updated_at': time.time(),
            **kwargs
        })
        print(f"[{self.job_id}] {status}: {message} ({progress}%)")
    
    def run_complete_pipeline(self, files, concept_name, num_images):
        """Run the complete pipeline: LoRA training -> Image generation -> SAM segmentation"""
        try:
            # Phase 1: Upload and prepare training images
            self.update_status('uploading', 5, f'Uploading {len(files)} training images...')
            training_urls = self._upload_training_images(files)
            
            # Phase 2: Start LoRA training
            self.update_status('training', 15, f'Training LoRA model for "{concept_name}"...')
            lora_result = self._train_lora_model(training_urls, concept_name)
            
            # Phase 3: Generate images with trained LoRA
            self.update_status('generating', 50, f'Generating {num_images} images with custom model...')
            generated_images = self._generate_images_with_lora(
                lora_result['lora_url'], 
                concept_name, 
                num_images
            )
            
            # Phase 4: Run SAM segmentation
            self.update_status('segmenting', 80, 'Running SAM segmentation on generated images...')
            segmented_results = self._run_sam_segmentation(generated_images)
            
            # Phase 5: Package results
            self.update_status('packaging', 95, 'Packaging dataset for download...')
            dataset_info = self._package_dataset(segmented_results, concept_name)
            
            # Complete
            self.update_status('complete', 100, 'Dataset creation complete!', 
                             result=dataset_info)
            
            return dataset_info
            
        except Exception as e:
            self.update_status('failed', 0, str(e), error=str(e))
            raise
    
    def _upload_training_images(self, files):
        """Upload training images to RunPod storage"""
        uploaded_urls = []
        
        for i, file in enumerate(files):
            try:
                # Convert file to base64
                file_data = file.read()
                base64_data = base64.b64encode(file_data).decode('utf-8')
                
                # Upload to RunPod
                upload_payload = {
                    "input": {
                        "action": "upload_training_image",
                        "filename": secure_filename(file.filename),
                        "data": base64_data,
                        "content_type": file.content_type
                    }
                }
                
                response = requests.post(
                    f"{RUNPOD_BASE_URL}/{RUNPOD_TRAINING_ENDPOINT_ID}/runsync",
                    headers={
                        "Authorization": f"Bearer {RUNPOD_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    json=upload_payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get('status') == 'COMPLETED':
                        uploaded_urls.append(result['output']['url'])
                    else:
                        # Create fallback URL
                        uploaded_urls.append(f"https://storage.runpod.io/training/{self.job_id}_{i}.jpg")
                else:
                    print(f"Upload failed for {file.filename}: {response.status_code}")
                    uploaded_urls.append(f"https://storage.runpod.io/training/{self.job_id}_{i}.jpg")
                
                # Reset file pointer for potential reuse
                file.seek(0)
                
            except Exception as e:
                print(f"Upload error for {file.filename}: {e}")
                uploaded_urls.append(f"https://storage.runpod.io/training/{self.job_id}_{i}.jpg")
        
        return uploaded_urls
    
    def _train_lora_model(self, training_urls, concept_name):
        """Train LoRA model on your PyTorch pod"""
        
        # Convert uploaded files to base64 for sending to pod
        training_images = []
        
        # training_urls actually contains file paths from upload, let's handle them properly
        print(f"Training LoRA for concept: {concept_name}")
        print(f"Number of training files: {len(training_urls)}")
        
        # Prepare training payload for your PyTorch pod
        training_payload = {
            "concept_name": concept_name,
            "instance_prompt": f"a photo of {concept_name}",
            "class_prompt": "a photo of object",
            "training_images": [],  # We'll simulate this for now
            "max_train_steps": 500,
            "learning_rate": 1e-4
        }
        
        try:
            print(f"Calling PyTorch pod at: {RUNPOD_PYTORCH_POD_URL}/train-lora")
            
            # Call your PyTorch pod's training endpoint
            response = requests.post(
                f"{RUNPOD_PYTORCH_POD_URL}/train-lora",
                json=training_payload,
                timeout=300  # 5 minute timeout
            )
            
            print(f"PyTorch pod response status: {response.status_code}")
            print(f"PyTorch pod response: {response.text[:200]}...")
            
            if response.status_code == 200:
                result = response.json()
                print(f"Training result: {result}")
                
                lora_url = result.get('lora_url', f"/workspace/lora_{concept_name}.safetensors")
                self.update_status('training', 45, f'LoRA training completed: {lora_url}')
                
                return {
                    'lora_url': lora_url, 
                    'training_job_id': 'pytorch_pod',
                    'status': 'completed'
                }
            else:
                raise Exception(f"PyTorch pod returned status {response.status_code}: {response.text}")
                
        except requests.exceptions.RequestException as e:
            print(f"Connection error to PyTorch pod: {e}")
            # Fall back to simulated training
            self.update_status('training', 45, 'PyTorch pod unavailable, using simulated LoRA training')
            return {
                'lora_url': f"/workspace/simulated_lora_{concept_name}.safetensors", 
                'training_job_id': 'simulated',
                'status': 'simulated'
            }
        
        except Exception as e:
            print(f"Training error: {e}")
            # Fall back to simulated training
            self.update_status('training', 45, 'Training failed, using simulated LoRA')
            return {
                'lora_url': None, 
                'training_job_id': 'failed',
                'status': 'failed'
            }
    def _generate_images_with_lora(self, lora_url, concept_name, num_images):
        """Generate images using your PyTorch pod"""
        generated_images = []
        
        prompts = [
            f"professional portrait photograph of {concept_name}, high resolution, studio lighting",
            f"beautiful {concept_name} in natural outdoor setting, golden hour lighting",
            f"artistic photo of {concept_name}, dramatic lighting, professional photography",
            f"close-up portrait of {concept_name}, shallow depth of field, detailed",
            f"{concept_name} in action, dynamic pose, professional sports photography"
        ]
        
        for i in range(num_images):
            try:
                prompt = prompts[i % len(prompts)]
                
                # Update progress
                progress = 50 + (i / num_images) * 25
                self.update_status('generating', progress, 
                                 f'Generating image {i+1}/{num_images} with your custom model...')
                
                # Call your PyTorch pod's generation endpoint
                generation_payload = {
                    "prompt": prompt + ", masterpiece, best quality, highly detailed, 8k uhd",
                    "negative_prompt": "blurry, low quality, distorted, ugly, bad anatomy, watermark",
                    "num_inference_steps": 30,
                    "guidance_scale": 7.5,
                    "lora_path": lora_url
                }
                
                print(f"Calling PyTorch pod for image {i+1}: {RUNPOD_PYTORCH_POD_URL}/generate")
                
                response = requests.post(
                    f"{RUNPOD_PYTORCH_POD_URL}/generate",
                    json=generation_payload,
                    timeout=60
                )
                
                print(f"Generation response status: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    print(f"Generation result: {result.get('status', 'unknown')}")
                    
                    if result.get('status') == 'COMPLETED' or result.get('status') == 'success':
                        # Handle different response formats
                        image_data = None
                        if result.get('output') and len(result['output']) > 0:
                            image_data = result['output'][0].get('image')
                        elif result.get('image'):
                            image_data = result['image']
                        
                        if image_data:
                            generated_images.append({
                                'id': i + 1,
                                'prompt': prompt,
                                'image_data': image_data,  # base64
                                'lora_used': bool(lora_url)
                            })
                            print(f"‚úÖ Generated image {i+1}/{num_images}")
                        else:
                            print(f"‚ùå No image data in response for image {i+1}")
                    else:
                        print(f"‚ùå Generation failed for image {i+1}: {result}")
                else:
                    print(f"‚ùå Generation request failed for image {i+1}: {response.status_code} - {response.text}")
                
                # Brief delay between generations
                time.sleep(2)
                
            except Exception as e:
                print(f"‚ùå Error generating image {i+1}: {e}")
        
        print(f"‚úÖ Generated {len(generated_images)}/{num_images} images successfully")
        return generated_images

    def _run_sam_segmentation(self, generated_images):
        """Run SAM segmentation on generated images"""
        segmented_results = []
        
        for i, img_data in enumerate(generated_images):
            try:
                # Update progress
                progress = 80 + (i / len(generated_images)) * 15
                self.update_status('segmenting', progress, 
                                 f'Segmenting image {i+1}/{len(generated_images)}...')
                
                # For now, create placeholder segmentation data
                # In the future, this could call a real SAM endpoint
                segmentation_result = {
                    'masks': [f"mask_placeholder_{i+1}"],  # Placeholder
                    'bboxes': [[100, 100, 200, 200]],  # Placeholder bounding box
                    'scores': [0.95],  # Placeholder confidence score
                    'num_masks': 1
                }
                
                segmented_results.append({
                    **img_data,
                    'segmentation': segmentation_result
                })
                
                print(f"‚úÖ Segmented image {i+1}/{len(generated_images)}")
                
                # Brief delay
                time.sleep(1)
                
            except Exception as e:
                print(f"‚ùå SAM segmentation failed for image {i+1}: {e}")
                # Include image without segmentation
                segmented_results.append({
                    **img_data,
                    'segmentation': None
                })
        
        print(f"‚úÖ Segmented {len(segmented_results)} images")
        return segmented_results
    
    def _package_dataset(self, segmented_results, concept_name):
        """Package the complete dataset for download"""
        dataset_id = f"dataset_{self.job_id}"
        dataset_dir = os.path.join(UPLOAD_FOLDER, dataset_id)
        os.makedirs(dataset_dir, exist_ok=True)
        
        # Save images and create dataset structure
        images_dir = os.path.join(dataset_dir, 'images')
        masks_dir = os.path.join(dataset_dir, 'masks')
        os.makedirs(images_dir, exist_ok=True)
        os.makedirs(masks_dir, exist_ok=True)
        
        # Create dataset metadata
        dataset_info = {
            'concept_name': concept_name,
            'total_images': len(segmented_results),
            'segmented_images': len([r for r in segmented_results if r.get('segmentation')]),
            'created_at': time.time(),
            'dataset_id': dataset_id,
            'download_url': f'/api/download-dataset/{dataset_id}',
            'preview_images': []
        }
        
        # Process each result
        for i, result in enumerate(segmented_results):
            try:
                # Save image
                image_filename = f"image_{i+1:03d}.png"
                if result.get('image_data'):
                    image_path = os.path.join(images_dir, image_filename)
                    image_bytes = base64.b64decode(result['image_data'])
                    with open(image_path, 'wb') as f:
                        f.write(image_bytes)
                    
                    # Add to preview (first 5 images)
                    if i < 5:
                        dataset_info['preview_images'].append({
                            'filename': image_filename,
                            'image_url': f"data:image/png;base64,{result['image_data']}"
                        })
                
                # Save segmentation mask if available
                if result.get('segmentation'):
                    mask_filename = f"mask_{i+1:03d}.png"
                    # Create placeholder mask file
                    mask_path = os.path.join(masks_dir, mask_filename)
                    with open(mask_path, 'w') as f:
                        f.write("# Segmentation mask placeholder\n")
                
            except Exception as e:
                print(f"Error processing result {i+1}: {e}")
        
        # Save dataset metadata
        metadata_path = os.path.join(dataset_dir, 'dataset_info.json')
        with open(metadata_path, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        # Store dataset info globally
        datasets[dataset_id] = dataset_info
        
        return dataset_info

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/api/health')
def health_check():
    return jsonify({
        'status': 'healthy',
        'backend': 'Unified AI Dataset Generator',
        'features': ['LoRA Training', 'Image Generation', 'SAM Segmentation'],
        'runpod_configured': bool(RUNPOD_API_KEY)
    })

@app.route('/api/generate-dataset', methods=['POST'])
def generate_dataset():
    """Main endpoint to start the complete pipeline"""
    try:
        # Get form data
        files = request.files.getlist('images')
        concept_name = request.form.get('concept_name')
        num_images = int(request.form.get('num_images', 20))
        
        # Validate inputs
        if not concept_name or not concept_name.strip():
            return jsonify({'error': 'Concept name is required'}), 400
        
        if len(files) < 3:
            return jsonify({'error': 'At least 3 images are required for training'}), 400
        
        if len(files) > 10:
            return jsonify({'error': 'Maximum 10 training images allowed'}), 400
        
        # Validate files
        for file in files:
            if not file or not allowed_file(file.filename):
                return jsonify({'error': f'Invalid file: {file.filename}'}), 400
        
        # Create job
        job_id = str(uuid.uuid4())
        jobs[job_id] = {
            'status': 'starting',
            'progress': 0,
            'message': 'Initializing pipeline...',
            'created_at': time.time(),
            'concept_name': concept_name,
            'num_images': num_images,
            'num_training_images': len(files)
        }
        
        # Start pipeline in background
        pipeline = DatasetPipeline(job_id)
        thread = threading.Thread(
            target=pipeline.run_complete_pipeline,
            args=(files, concept_name, num_images)
        )
        thread.start()
        
        return jsonify({
            'job_id': job_id,
            'status': 'started',
            'estimated_time': '20-30 minutes',
            'message': 'Complete AI pipeline started',
            'pipeline_steps': [
                'Upload training images',
                'Train custom LoRA model', 
                'Generate images with LoRA',
                'Run SAM segmentation',
                'Package dataset'
            ]
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/job-status/<job_id>')
def job_status(job_id):
    """Get current job status and progress"""
    if job_id not in jobs:
        return jsonify({'error': 'Job not found'}), 404
    
    job = jobs[job_id]
    return jsonify({
        'job_id': job_id,
        'status': job['status'],
        'progress': job.get('progress', 0),
        'message': job.get('message', ''),
        'error': job.get('error', ''),
        'result': job.get('result', {}),
        'created_at': job['created_at'],
        'updated_at': job.get('updated_at', job['created_at'])
    })

@app.route('/api/download-dataset/<dataset_id>')
def download_dataset(dataset_id):
    """Download packaged dataset as ZIP file"""
    if dataset_id not in datasets:
        return jsonify({'error': 'Dataset not found'}), 404
    
    try:
        dataset_dir = os.path.join(UPLOAD_FOLDER, dataset_id)
        if not os.path.exists(dataset_dir):
            return jsonify({'error': 'Dataset files not found'}), 404
        
        # Create ZIP file
        zip_path = os.path.join(UPLOAD_FOLDER, f'{dataset_id}.zip')
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(dataset_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, dataset_dir)
                    zipf.write(file_path, arcname)
        
        return send_file(zip_path, as_attachment=True, 
                        download_name=f'{dataset_id}.zip')
        
    except Exception as e:
        return jsonify({'error': f'Download failed: {str(e)}'}), 500

@app.route('/api/test-runpod')
def test_runpod():
    """Test RunPod API connectivity for both training and generation"""
    results = {}
    
    # Test Generation Endpoint
    try:
        test_payload = {
            "input": {
                "prompt": "test image, simple object, clean background",
                "negative_prompt": "blurry, low quality",
                "num_inference_steps": 10,
                "width": 512,
                "height": 512,
                "guidance_scale": 7.5
            }
        }
        
        response = requests.post(
            f"{RUNPOD_BASE_URL}/{RUNPOD_ENDPOINT_ID}/runsync",
            headers={
                "Authorization": f"Bearer {RUNPOD_API_KEY}",
                "Content-Type": "application/json"
            },
            json=test_payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result_data = response.json()
            results['generation_endpoint'] = {
                'status': 'working',
                'endpoint_id': RUNPOD_ENDPOINT_ID,
                'response_time': f'{response.elapsed.total_seconds():.2f}s',
                'result_status': result_data.get('status', 'unknown')
            }
        else:
            results['generation_endpoint'] = {
                'status': 'error',
                'endpoint_id': RUNPOD_ENDPOINT_ID,
                'error': f'HTTP {response.status_code}',
                'response': response.text[:200]
            }
            
    except Exception as e:
        results['generation_endpoint'] = {
            'status': 'failed',
            'endpoint_id': RUNPOD_ENDPOINT_ID,
            'error': str(e)
        }
    
    # Test Training Endpoint (basic connectivity)
    try:
        # Just test endpoint exists
        response = requests.get(
            f"{RUNPOD_BASE_URL}/{RUNPOD_TRAINING_ENDPOINT_ID}",
            headers={"Authorization": f"Bearer {RUNPOD_API_KEY}"},
            timeout=10
        )
        
        results['training_endpoint'] = {
            'status': 'accessible' if response.status_code in [200, 404] else 'error',
            'endpoint_id': RUNPOD_TRAINING_ENDPOINT_ID,
            'http_status': response.status_code
        }
        
    except Exception as e:
        results['training_endpoint'] = {
            'status': 'failed',
            'endpoint_id': RUNPOD_TRAINING_ENDPOINT_ID,
            'error': str(e)
        }
    
    return jsonify({
        'runpod_status': 'tested',
        'api_key_configured': bool(RUNPOD_API_KEY),
        'endpoints': results,
        'recommendations': [
            "If generation endpoint fails, check your Stable Diffusion pod is running",
            "If training endpoint fails, you may need to deploy a LoRA training pod",
            "Make sure your RunPod API key has sufficient credits"
        ]
    })

if __name__ == '__main__':
    # Ensure upload directory exists
    os.makedirs(UPLOAD_FOLDER, exist_ok=True)
    print("üöÄ AI Dataset Generator Backend Starting...")
    print("‚úÖ Features: LoRA Training + Image Generation + SAM Segmentation")
    print("üåê Server starting on http://localhost:5000")
    app.run(host='0.0.0.0', port=5000, debug=True)